## 视觉几何

SLAM 3D视觉 秋招面经 

https://www.nowcoder.com/discuss/294308?type=2&order=0&pos=28&page=1

https://zhuanlan.zhihu.com/p/46694678

https://blog.csdn.net/weixin_44580210/article/details/91790044

https://blog.nowcoder.net/n/1b4e099dc9ef4ff094a562e752e125cf

https://blog.csdn.net/electech6/article/details/95226909

https://www.cnblogs.com/xtl9/p/8053331.html

https://zhuanlan.zhihu.com/p/63755692

https://github.com/huihut/interview

https://juejin.im/post/5d1b56135188252bdb148690

https://www.w3cschool.cn/cpp/cpp-a9no2ppi.html



### Q: 三个角的名称

俯仰角 pitch 绕X轴，航向角 yaw 绕Y轴，横滚角 roll 绕 Z 轴。

http://yuenshome.space/timeline/2018-11/face-pose-estimation/

###  Q: 相机坐标系转换, 相机内参的含义

https://blog.csdn.net/chentravelling/article/details/53558096

图像坐标系到像素坐标系：缩放+平移 u = ax+b, v = cy+d，写成矩阵形式就是**内参**。如果没有矫正过图像坐标系和像素坐标系的xy轴会不平行，反应在内参上就是K(1,0)和K(0,1)也不为0。

相机坐标系到图像坐标系：透视投影，直接除以深度 x = Pc / Zc，x 为二维图像坐标系下的齐次坐标，通常单位为mm。

相机坐标系到世界坐标系：外参。

### Q: 双目估计深度推导

https://zhuanlan.zhihu.com/p/32199990

相机坐标系到成像平面的焦距为 f，左右相机间距 b，

根据相似三角形: $\frac{z}{f} = \frac{x}{u_l}$, $\frac{z}{f} = \frac{x - b}{u_r}$，整理得 $u_lz = xf$, $u_rz = (x-b)f$，解得 $z = \frac{fb}{u_l - u_r}$。

### Q: 双目深度误差推导

TODO

### Q: Depth Flow Ego-motion 转换关系

关键点在于记住像素到成像的转换为 `x = K_inv * p`，成像到相机的转换为`x_c = d * x` ，相机到世界的转换 `x_w = T * x_c`。

其中 `d` 为标量， `p` 和 `x` 都是齐次坐标系下的 2d 坐标向量，`x_c`, `x_w` 是 3d 坐标向量。

Flow Depth 到 Pose: PnP 优化目标: $||p_1 + F - Kd_1TK^{-1}p_1/d_2||$，最小二乘，维度为点的个数。注意其中 $d_2$ 不需要已知，直接等于算出来的$\hat{p}_2$的第三维。

Depth + Pose 到 Flow: 直接计算获得刚体流：$Kd_1TK^{-1}p_1/d_2 – p_1$，注意其中 $d_2$ 不需要已知，直接等于算出来的 $\hat{p}_2$ 的第三维。

Flow + Pose 到 Depth: 

1. 建立方程 $d_1RK^{-1}p_1 + t = d_2K^{-1}(p_1 + F)$。
2. 右边 $K^{-1}(p_1+F)$ 为成像坐标系点记做 $x_2$，左边 $K^{-1}p_1$ 记做 $x_1$。
3. 方程即为 $d_1 R x_1 +t = d_2 x_2$，左右同左乘 $x_2^{\land}$，$d_1x_2^{\land}Rx_1 + x_2^{\land}t=0$ ，求得 $d_1$，再去求 $d_2$。

https://xhy3054.github.io/triangulation/

https://www.zhihu.com/question/23418797

### Q: PnP原理，是否是凸的，优化方法。

PnP求解的是已知世界坐标系下的点和相机坐标系下投影点的位置时的3D-2D相机位姿估计问题，不需要对极约束(存在初始化，纯旋转和尺度问题，且一般需要8对点)。

优化目标: ||p2 - KD1(p1)TK^(-1)p1||

PnP和ICP之类的都是解超定线性(如果距离的度量是线性的话)方程组，本质是求解最小二乘，是凸优化，优化方法可以是SVD解最小二乘(用的数据太多了)，可以是P3P，DLT，EPnP。P3P方法是通过3对3D/2D匹配点，求解出四种可能的姿态，但是对于相机远离3D平面（low parallax）或者视角垂直于3D平面的情况下效果不佳。

https://www.jianshu.com/p/b3e9fb2ad0dc

EPnP：需要4对不共面的（对于共面的情况只需要3对）3D-2D匹配点，是目前最有效的PnP求解方法。

PnP用于人脸姿态估计:

PnP或ICP为什么要用RANSAC: 因为数据里存在不确定的外点，RANSAC通过采样来估计出去除外点后的估计。

凸函数的定义: 如果一个函数是一个凸函数，那么该函数两点的连线必然在该函数图形的上方，凸函数的局部最优点就是它的全局最优点。

最小二乘求解的是 min |y - AX + b|^2，这个超定方程，在X没有非线性的时候是凸的，有全局最优解。

### Q. 如何求解线性方程Ax=b？SVD和QR、LU分解哪个更快？

https://www.zhihu.com/question/22572629

LU需要可逆的条件，QR速度快但是没有SVD稳定，SVD是目前求解最小二乘最好的矩阵分解法。

### Q. 图像矫正是在矫正什么，单目畸变矫正和双目平行矫正的区别

https://zhuanlan.zhihu.com/p/32199990

https://blog.csdn.net/cuglxw/article/details/77885572

https://blog.csdn.net/LoseInVain/article/details/102775734

### Q. 传统的 Stereo/Flow 方法最优的是什么，流程，优化方法。

SGM, PSMNet, RAFT, PDHG

[HS]Determining Optical Flow

连续优化（HS，tv-l1）离散优化(DCFlow)

https://cloud.tencent.com/developer/article/1432358

https://wenku.baidu.com/view/a82065052cc58bd63086bd0c

https://blog.csdn.net/qq_36880027/article/details/105361471

TV-L1 的意思是 (L1 data term and total variation regularization)Total-Variation model with l1 minimization problem，使用Primal-dual hybrid gradient (一种一阶优化器)进行优化。

### Q. MRFlow 和 TV-L1 速度/精度差异的原因



### Q. 什么是极线约束



### 双目的缺点：

视觉应该是激光雷达方案的一个补充，目前自动驾驶很少使用双目的配置了，如waymo, nuscenes数据集都采用的环视加Lidar的配置，原因如下。

\-     双目基于可见光，晚上就瞎了；激光雷达不会

\-     双目测距精度与标定有关且强相关，但是装在车上机械结构稳定性太差，面临着隔断时间就得标定的问题，当然激光雷达用久了也得换，也得和其它硬件重新标定；

\-     测距精度低且依赖项较多，其距离 [公式] ，Z是距离，F是相机焦距（单位为pixel），b为双目相机两个相机的基线距（光心距离），测距绝对误差 [公式] ，算法固定的情况下，想要测距精度高，就得增大f或者b，焦距大了，视场角变小了，基线距大了，能看到的最近距离就更远了；

\-     视场角太小，广角镜头（标定完100度撑死了，参考zed双目相机），激光雷达360度。

\-     算法层面，双目是通过左右匹配（纹理特征）算出来的，面对无纹理、重复纹理场景直接gg了，别提深度学习啊，那么多场景，你能保证你的训练样本都能cover 的了吗？但是激光雷达这些完全没影响啊。

\-     本身感知定位决策加起来就有几十毫秒甚至过百毫秒的延迟了，再加个计算视差图，可能做出决策的时候已经撞车了吧。

### 传统双目的流程

匹配代价计算、代价聚合、视差优化、和后处理。匹配代价计算为左图像块和可能对应的右图像块提供初始相似性度量，这一步对立体匹配很关键。一些常用的匹配代价包括绝对差值（SAD）、平方差之和（SSD）、归一化相关系数（NCC）。

### 深度学习双目估计

PSM-Net: Follow https://arxiv.org/pdf/1703.04309.pdf，用SPP后的左右视图特征，直接错位cat起来得到HWDF的四维特征,用3D卷积回归视差。

Group-wise Correlation Stereo Network，分组构建cost volume, 320个channel分40组，组内计算correlation，得到HWDG四维tensor, 用3D卷积回归视差。

### Q: 双目的优点：

用长焦 + 大焦距(要求卡车等车宽大的，也可以前后布置，见cvpr2020港科大paper) + 激光 + 深度学习可以做到小视角的超远距离探测。

如何衡量自监督学习方法的性能

https://www.keoaeic.org/computer/7891.html

\-     用一个线性分类器对无监督学习得到的特征进行监督分类，但这非常依赖学习率的调整策略。SimCLR 4x +线性分类器能达到ImageNet 76.5的精度。

\-     全量或少量监督数据的Fine-tuning，

\-     迁移到其他任务测试。

### Q: 给定像素点, 内参, 外参, 以及像素所在的平面，求像素的世界坐标系点

像素点x=[u, v]，内参K，外参R, T, 平面为(点法式过p0, 法向量为n)

x = [u, v, 1]，世界坐标系点为dRK^-1x + T，但注意深度为止，需要通过点和平面的交点求得。

[https://zh.wikipedia.org/wiki/%E7%BA%BF%E9%9D%A2%E4%BA%A4%E7%82%B9](https://zh.wikipedia.org/wiki/线面交点)

平面点法式为 (p – p0)n = 0，直线为过相机坐标系原点与相点的直线，p=cK^-1x，c为待求标量，物理含义就是深度值。两式联立可以得 cK^-1xn – p0n = 0，解得 c = p0n/K^-1xn。

### Q: 二维矩阵顺时针旋转90°以及旋转任意度。

https://blog.csdn.net/sinat_33425327/article/details/78333946

http://www.skcircle.com/?id=653(图很有参考性，但是乘法的地方方向不太符合常理)

设点A(x0, y0)，以列, 行描述且与x轴夹角记为a，绕原点顺时针旋转 b 度，得到B点(x1, y1)。

二维矩阵旋转时，绕点为矩阵中心，记做O=(w/2, h/2)，则在O坐标系下原矩阵的左上角(0,0)点为OA=(-w/2,h/2)，旋转后矩阵左上角点记做A’=(0,0)，对于某一点B’=(x,y)，先把坐标系移到以O为原点，则OB’=OA’+A’B’=(旋转b度)OA + (x,y)= (旋转b度)(-w/2,h/2) + (x,y)。

再计算OB点坐标，即(旋转-b度)OB’，再计算B在A坐标系下的坐标AB=OB-OA=OB+(w/2, -h/2)。

有方程 r = x0/cos(a) = y0/sin(a) = x1/cos(a-b) = y1/sin(a-b)，

则 x1 = r*cos(a-b) = r*cos(a)cos(b) + r*sin(a)sin(b)=x0cos(b) + y0sin(b)，

y1 = r*sin(a-b) = r*sin(a)cos(b) - r*cos(a)sin(b)=y0cos(b) - x0sin(b)。

整理得: x1 = x0cos(b) + y0sin(b), y1 = y0cos(b) - x0sin(b)。

一个点(x,y)绕另一个点(tx,ty)旋转：

![img](D:\Notes\Interview\clip_image020.jpg)



