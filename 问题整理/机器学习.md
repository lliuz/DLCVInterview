### 二维平面，一堆散点，找一个条拟合的直线/曲线/分布函数

直线：线性回归，最小二乘。

曲线：多项式回归，$x_1=1，x_2=x，x_3=x^2 ...$，选择好几次多项式，再转换成矩阵形式最小二乘。

分布函数：极大似然估计，假设分布形式为 p(x)，在观测样本下的联合概率为 p(x1)p(x2)...p(xn)，取对数把连乘转累加即似然函数：

L = ln(p(x1)) + ln(p(x2)) + ... + ln(p(xn))，令似然函数最大，求解得到分布参数的值就是拟合的分布函数。

### Q: Kmeans 进阶

https://www.jianshu.com/p/96e05bd28171

https://www.cnblogs.com/yixuan-xu/p/6272208.html

\-     K-means++:假设已经选取了n个初始聚类中心(0<n<K)，则在选取第n+1个聚类中心时：距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。在选取第一个聚类中心(n=1)时同样通过随机的方法。

\-     ISODATA: 当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多。分散程度较大时把这个类别分为两个子类别。

\-     Kernel K-means: 不再使用欧氏距离作为距离度量。

\-     分布式K-means: 主节点把点分组发给各个从节点，主节点选择聚类中心点，发给各个从节点，从节点对得到的点计算距离并将每个点的聚类结果(用于计算新的中心点)和距离之和(用于判断是否收敛)返回主节点，主节点汇总结果，计算新中心点并判断是否收敛。

### 朴素贝叶斯简述

朴素贝叶斯通过预测指定样本属于特定类别的条件概率 $P(y_i|x)$ 来预测样本所属类别，即
$$
y = \max_i P(y_i | x)
$$
其中 $y$ 的条件概率可以由贝叶斯公式表示为
$$
P\left(y_{i} \mid x\right)=\frac{P\left(x \mid y_{i}\right) P\left(y_{i}\right)}{P(x)}
$$
其中分类时，$P(x)$ 为常量，$P(y_i)$ 通常由统计取得，也就是预测类别 $y$ 由似然度 $P(x|y_i)$ 来决定。

> Pr(A)是A的[先验概率](https://baike.baidu.com/item/先验概率)或边缘概率。之所以称为"先验"是因为它不考虑任何B方面的因素。
>
> Pr(A|B)是已知B发生后A的[条件概率](https://baike.baidu.com/item/条件概率)，也由于得自B的取值而被称作A的[后验概率](https://baike.baidu.com/item/后验概率)。
>
> Pr(B|A)是已知A发生后B的条件概率，也由于得自A的取值而被称作B的后验概率。
>
> Pr(B)是B的先验概率或边缘概率，也作标准化常量（normalized constant）。
>
> **按这些术语，Bayes法则可表述为：**
>
> 后验概率 = (似然度 * 先验概率)/标准化常量　也就是说，后验概率与先验概率和似然度的乘积成正比。

### 为什么朴素贝叶斯如此朴素

因为在朴素贝叶斯中假设特征独立，即$P(x|y_i)$ = $\prod_j P(x_j|y_i)$，这个假设太强了，因此说朴素贝叶斯很“朴素”。

为什么需要假设特征之间相互独立呢，如果不独立的话 $P(x|y_i)$ 的建模会非常困难。

https://zhuanlan.zhihu.com/p/26262151

https://cloud.tencent.com/developer/article/1368310

### 生成模型与判别模型

生成模型是对联合概率建模，通过计算边缘概率得到预测 $P(y|x) = P(xy)/P(x)$，而判别模型是直接对条件概率 $P(y|x)$ 建模。

CRF 是判别模型，混合高斯模型 GMM，朴素贝叶斯，HMM都是生成模型。



boost、Adaboost

什么是SVM

SVM 、核函数

过拟合 欠拟合 

bias variance

### 