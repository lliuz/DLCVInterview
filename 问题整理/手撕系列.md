### Focal Loss

https://github.com/fudannlp16/focal-loss/blob/master/focal_loss.py

```python
def focal_loss_sigmoid(labels, logits, alpha=0.25, gamma=2):
    """
    Computer focal loss for binary classification 二分类
      - labels: N
      - logits: N
    Returns:
      - 每个样本的 focal loss, A tensor of the same shape as `lables`
    """
    y_pred = tf.nn.sigmoid(logits)
    labels = tf.to_float(labels)
    L = -(labels * (1 - alpha) * ((1 - y_pred) * gamma) * tf.log(y_pred) +
          (1 - labels) * alpha * (y_pred ** gamma) * tf.log(1 - y_pred))
    return L

def focal_loss_softmax(labels, logits, gamma=2):
    """
    Computer focal loss for multi classification
      - labels: N 非 one-hot 编码
      - logits: NxC
    Returns:
      - 每个样本的 focal loss, A tensor of the same shape as `lables`
    """
    y_pred = tf.nn.softmax(logits, dim=-1)  # [batch_size,num_classes]
    labels = tf.one_hot(labels, depth=y_pred.shape[1])
    L = -labels * ((1 - y_pred) ** gamma) * tf.log(y_pred)
    L = tf.reduce_sum(L, axis=1)
    return L

```

### Batch Norm

```python
def batch_norm(x, gamma, beta):
    if is_train:
        mean = np.mean(x, axis=[0, 2, 3)
        var = np.var(x, axis=[0, 2, 3])        
        runing_mean = momentum * runing_mean + (1 - momentum) * mean	# 初始化为 0
        runing_val = momentum * runing_val + (1 - momentum) * val		# 初始化为 1
    else:
        mean = runing_mean
        val = runing_val
    x_hat = (x - mean) / np.sqrt(var + eps)
    y = x_hat * gamma + beta
    return y
```

### IoU 

关键点在于交集面积 = `max(0, rb0 - lt0) * max(0, rb1 - lt1)`, 分别是左上角的最大值乘右下角的最小值。

IoU = 交集 / (面积和 - 交集)

```python
def IoU_one2one(box1, box2):
    # 一对一
    # x_lt, y_lt, x_rb, y_rb
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])

    lt0 = max(box1[0], box2[0])
    lt1 = max(box1[1], box2[1])

    rb0 = min(box1[2], box2[2])
    rb1 = min(box1[3], box2[3])

    intersect = max(0, rb0 - lt0) * max(0, rb1 - lt1)
    iou = intersect / (area1 + area2 - intersect + eps)
    return iou

def IoU_one2all(box, boxes):
    # 一对多, 利用广播
    area1 = (box[2] - box[0]) * (box[3] - box[1])
    area2 = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])

    lt0 = np.maximum(box[0], boxes[:, 0])
    lt1 = np.maximum(box[1], boxes[:, 1])
    rb0 = np.minimum(box[2], boxes[:, 2])
    rb1 = np.minimum(box[3], boxes[:, 3])
    
    intersect = max(0, rb0 - lt0) * max(0, rb1 - lt1)
    iou = intersect / (area1 + area2 - intersect + eps)
    return iou

def IoU_all2all(boxes1, boxes2):
    # 求 dense pair-wise IoU, 得益于GPU的并行计算，我们可以一次性得到IoU的全部计算结果。这一步就已经极大地解决了IoU计算繁琐又耗时的问题。
    
    def box_area(box):
        # box = 4xn
        return (box[2] - box[0]) * (box[3] - box[1])

    area1 = box_area(boxes1.t())
    area2 = box_area(boxes2.t())
    
    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2]) # [N,M,2] 
    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:]) # [N,M,2]
	inter = (rb - lt).clamp(min=0).prod(2) # [N,M] 
    return inter / (area1[:, None] + area2 - inter)
    
```

### NMS

这里给出的是先排序后去重的方法，由于时间复杂度的大头都在循环部分，所以这种写法的时间复杂度和 softmax 中循环内取最大是一致的。

```python
def nms(boxes, thresh):
    order = boxes[:, 4].argsort()	# 从 score 低到高排序
    keep = []  # 用于存放最后的结果
    while order: # 循环，直至order中没有元素
        keep.append(boxes[order.pop()])				# 读取分数最大的box的索引并保留
        iou = IoU_one2all(keep[-1], boxes[order])	# 注意这里需要拿 box 和 order 排序后的 boxes 计算，从而下一步可以直接取。
        order = order[iou <= thresh]   
return keep
```

### Soft-NMS

Soft-NMS 通过衰减与得分高的检测框有重叠的框的置信度，从而滤去一些 FP，保留一些框避免FN。

原版NMS可以认为将重叠率超过阈值的得分设为0，Soft-NMS则是得分改为线性加权 $s_i(1-IoU(M,b_i))$ 或者 高斯加权 $s_ie^-\frac{IoU()^2}{\sigma}$.

注意：

- Soft-NMS 和 原版 NMS 一致，本质上都是一种贪心算法，并不能保证找到全局最优的检测框分数重置。
- 原版NMS可以一次排序(循环外部 O(nlog))，Soft-NMS需要每次求最大(循环内部为 O(n))，但总体时间复杂度都是O(n^2)

```python
def soft_nms(boxes, thresh, sigma=0.5):
    keep = []  # 用于存放最后的结果
    while boxes: # 循环，直至 box 中没有元素
        keep.append(boxes.pop(boxes[:, 4].argmax()))	# 读取分数最大的box的索引并保留
        iou = IoU_one2all(keep[-1], boxes)
        boxes[:, 4] = boxes[:, 4] * np.exp(-(iou * iou) / sigma)	# 把下面两句改成 boxes = boxes[iou <= thresh, :] 就是原版
		boxes = boxes[boxes[:, 4] > thresh, :]
return keep
```

### 加速 NMS

**Fast NMS**

https://0980da60.wiz03.com/wapp/pages/view/share/s/09wdFw0SO17G2tefvc22udtf1WchRv3EVQTZ2yOpF51G-mug

会抑制更多的框，性能略微下降，但在特定任务(如实例分割)上可以用。

```python
def fast_nms(self, boxes, scores, NMS_threshold:float=0.5):
    # scores, idx = scores.sort(1, descending=True)
    # boxes = boxes[idx]   # 对框按得分降序排列
    iou = IoU_all2all(boxes, boxes)  # IoU矩阵
    iou.triu_(diagonal=1)  # 上三角化
    keep = iou.max(dim=0)[0] < NMS_threshold  # 列最大值向量，二值化

    return boxes[keep], scores[keep]
```

**Cluster NMS**



**Matrix NMS**



### K-means

````python
def kmeans(X, K):
    # X: N个样本, C维
    N, C = X.shape
    centers = X[np.random.choice(N, K)]  # 随机选择k个初始中心点

    while True:
        # NxC - Kx1xC -> KxNxC -> KxN
        distances = np.sqrt(((X - centers[:, np.newaxis]) ** 2).sum(axis=2))
        closest = np.argmin(distances, axis=0)  # N

        new_centers = np.array([X[closest == k].mean(axis=0) for k in range(K)])

        if np.allclose(new_centers, centers):
            return new_centers
        centers = new_centers
````

### 逻辑回归

https://www.lagou.com/lgeduarticle/66470.html

```python
def logistic(X, y, W, b):
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    N = X.shape[0]
    # forward
    a = sigmoid(np.dot(X, W) + b)	
    cost = -1 / N * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))
    # backward
    dW = np.dot(X.T, (a - y)) / N
    db = np.sum(a - y) / N
    cost = np.squeeze(cost)
    return a, cost, dW, db
```



