https://www.cnblogs.com/pinard/p/6239403.html#4414913

PCA 是为了降低特征的维度且减少特征之间的相关性。问题被转化成了，我们希望找出一组基向量（需要是标准正交基），数据在这组基向量构成的空间中表示时，任何两个属性间相关性为零。

方差定义数据的离散程度：
$$
\operatorname{Var}(a)=\frac{1}{m} \sum_{i=1}^{m}\left(a_{i}-\mu\right)^{2}
$$
协方差表示两个向量之间的相关性：
$$
\operatorname{Cov}(a, b)=\frac{1}{m} \sum_{i=1}^{m}\left(a_{i}-\mu_{a}\right)\left(b_{i}-\mu_{b}\right)
$$

1. **各个维度减去各自的均值**

   这一步是因为后面求方差和协方差时会要减均值，干脆提前减了，方便矩阵操作。

   减均值之后，方差和协方差就表示为：
   $$
   \begin{aligned}
   \operatorname{Var}(a) &=\frac{1}{m} \sum_{i=1}^{m} a_{i}^{2} \\
   \operatorname{Cov}(a, b) &=\frac{1}{m} \sum_{i=1}^{m} a_{i} b_{i}
   \end{aligned}
   $$

2. **计算数据矩阵的协方差矩阵（多个输入需要求平均）**

   对于 $n$ 条 $d$ 维数据的数据矩阵 $A_{n \times d}$，其协方差矩阵 $C$ 的计算为：
   $$
   C = \frac{1}{m} A^T  \cdot  A
   $$
   协方差阵 $C_{d \times d}$ 是一个实对称矩阵，对角线上的值是其方差，其他值对应两个属性之间的协方差。

   > 这里的维度定义按深度学习常用的顺序和符号来，其它文章里可能会用不一样的定义，比如 dxn 或者 mxn，导致下面的推导会有一些些不一样，比如可能其它文章最后会用 SVD 的 U 矩阵，不影响理解。

3. **协方差矩阵对角化 **(证明)

   我们的目的是找到一个新的基向量表示的空间 $P_{d \times d}$ ，使数据 $A$ 在 $P$ 下的新表示 $Z = AP$ 的各个属性相关性最小，也就是 $Z$ 的协方差矩阵 $\frac{1}{m} Z^{T} \cdot Z$ 是对角阵 (除对角线外其余元素为0)。

   这涉及到一个优化问题：如何找到 $P$ 使得 $Z = AP$ 的协方差矩阵是对角阵，

   我们把 $Z$ 的协方差矩阵展开：
   $$
   \begin{aligned}
   \frac{1}{m} Z^{T} Z &=\frac{1}{m}(A P)^{T} A P \\
                       &=\frac{1}{m} P^{T} A^{T} A P \\
   &=P^{T}\left(\frac{1}{m} A^{T} A\right) P \\
   & = P^T C P
   \end{aligned}
   $$
   优化问题重写为：
   $$
   \begin{aligned}
   &\arg\max_{P} \operatorname{tr}\left(P^TCP \right)\\
   &\text { s.t. } \quad P^T P=I
   \end{aligned}
   $$
   使用拉格朗日乘子法：
   $$
   J(P) = \operatorname{tr}(P^TCP) + \lambda(P^TP - I)
   $$
   对 $P$ 求导可得：
   $$
   CP + \lambda P = 0
   $$
   也就是说 $P$ 矩阵是 $C$ 矩阵的特征向量组成的矩阵。

4. **协方差矩阵对角化 **(操作)

   求协方差矩阵 $C = \frac{1}{m} A^TA$ 的特征向量可以暴力求解，但是一般是通过 SVD 求解的(可以利用一些快速 SVD 算法绕过求协方差矩阵特征向量的步骤)。

   因为 SVD 中的右奇异矩阵 $V$ 就是 $A^T A$ 的特征向量组成的矩阵，也就是说如果数据矩阵 $A$ 提前除了 $\frac{1}{\sqrt{m}}$，那么 SVD 分解后的矩阵 $V$ 就是我们要求的投影矩阵 $P$。

   - $AV^T$ 得到新的特征的数值;
   - 如果需要降维，按特征值从大到小排列，选取前 $k$ 个特征值，重新计算特征向量即可。

### PCA 的变种

作为一个非监督学习的降维方法，PCA 只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，如解决非线性降维的 KPCA，解决内存限制的增量 PCA 方法 Incremental PCA，以及解决稀疏数据降维的 PCA 方法 Sparse PCA 等。

#### **核主成分分析 KPCA**

在上面的 PCA 算法中，我们假设存在一个线性的超平面，可以让我们对数据进行投影。但是有些时候，数据不是线性的，不能直接进行 PCA 降维。

这里就需要用到和支持向量机一样的核函数的思想，先把数据集从n维映射到线性可分的高维 $N>n$ ,然后再从N维降维到一个低维度 $n'$, 这里的维度之间满足 $n'<n<N$。

具体的操作流程就是先对数据矩阵 $A$ 套一个核函数，再执行 PCA。

### PCA 优缺点

PCA 算法的主要优点有：

1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　

2. 各主成分之间正交，可消除原始数据成分间的相互影响的因素。

3. 计算方法简单，主要运算是特征值分解，易于实现。

PCA 算法的主要缺点有：

1. 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。

2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

